%\documentclass[a4paper,twocolumn]{article} % Document type

\documentclass[a4paper,12pt,oneside,onecolumn]{article} % Document type

\usepackage[left=1.0in, right=1.0in, top=1.0in, bottom=1.0in]{geometry}

\ifx\pdfoutput\undefined
    %Use old Latex if PDFLatex does not work
   \usepackage[dvips]{graphicx}% To get graphics working
   \DeclareGraphicsExtensions{.eps} % Encapsulated PostScript
 \else
    %Use PDFLatex
   \usepackage[pdftex]{graphicx}% To get graphics working
   \DeclareGraphicsExtensions{.pdf,.jpg,.png,.mps} % Portable Document Format, Joint Photographic Experts Group, Portable Network Graphics, MetaPost
   \pdfcompresslevel=9
\fi

\usepackage{amsmath,amssymb}   % Contains mathematical symbols
\usepackage[ansinew]{inputenc} % Input encoding, identical to Windows 1252
\usepackage[english]{babel}    % Language
\usepackage[square,numbers]{natbib}     %Nice numbered citations
\bibliographystyle{plainnat}            %Sorted bibliography



\begin{document}               % Begins the document

\title{Paperwork on Degree Project: Machine Learning Based Fault Prediction for Real-time Scheduling on Shop-floor}
\author{
  First Name 1 Last Name 1 \\ Personal Number 1 \\ E-mail 1 
  \and 
  First Name 2 Last Name 2 \\ Personal Number 2 \\ E-mail 2
  }
%\date{2010-10-10}             % If you want to set the date yourself.

\maketitle                     % Generates the title




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Instructions regarding the report
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Hopfield Network}

\section*{Overview}

Deep belief nets(DBNs) are probabilistic generative models that are composed of multiple layers of stochastic, latent variables. The latent variables typically have binary values and are often called hidden units or feature detectors. The top two layers have undirected, symmetric connections between them and form an associative memory. The lower layers receive top-down, directed connections from the layer above. The states of the units in the lowest layer represent a data vector.(Geoffrey E. Hinton (2009) Deep belief networks. Scholarpedia, 4(5):5947.)[1] DBN can be viewed as a composition of simple, unsupervised networks like restricted Boltzmann machines(RBMs) or auto-encoders. In the structure of DBN, each sub-network's hidden layer serves as the visible layer for the next. This structure leads to a layer-by-layer fast training process.\\
Deep belief nets have been widely used for image and video sequence recognition, as well as fault diagnosis and prediction in other fields. For example, Yan Liu et al. developed a discriminative DBN for visual data classification which outperforms both representative semi-supervised classifiers and existing deep learning techniques(Pattern Recognition)[2]. Hai B. Huang et al. combined regression-based DBN with SVM and performed sound quality prediction of vehicle interior noise with their model where the result shows the combined model outperforms four conventional machine learning methods multiple linear regression(MLR), back-propagation neural network(BPNN), general regression neural network(GRNN) and SVM(Applied Acoustic)[3]. Furao Shen et al. used DBN for exchange rate forecasting in finance and found their model better than typical forecasting methods such as feed forward neural network (FFNN)(Neurocomputing)[4]. De-long Feng et al. developed a DBN model for  fault-diagnosis simulation study of gas turbine engine(Frontiers of Information Technology and Electronic Engineering)[5]. 

\section*{Algorithm Details}
Overall, fast training, high model complexity, require large amount of training data. Capable of learning complex patterns. Problem in fine tuning.\\
Step 1:\\
...\\
Step n:
equations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliography{Bibliography_template} %Read the bibliography from a separate file

\begin{thebibliography}{99}
\bibitem[Khalil(2002)]{Khalil:2002:Nonlinear-systems:vh}
Hassan~K Khalil.
\newblock \emph{Nonlinear systems}.
\newblock Prentice Hall, Upper Saddle river, 3. edition, 2002.
\newblock ISBN 0-13-067389-7.

\bibitem[Oetiker et~al.(2008)Oetiker, Partl, Hyna, and
  Schlegl]{Oetiker:2008:TheNotSoShortIntroductiontoLaTeXe}
Tobias Oetiker, Hubert Partl, Irene Hyna, and Elisabeth Schlegl.
\newblock \emph{The Not So Short Introduction to \LaTeXe}.
\newblock Oetiker, OETIKER+PARTNER AG, Aarweg 15, 4600 Olten, Switzerland,
  2008.
\newblock http://www.ctan.org/info/lshort/.

\bibitem[Sastry(1999)]{Sastry:1999:Nonlinear-systems:-analysis-stability-and-c%
ontrol:xr}
Shankar Sastry.
\newblock \emph{Nonlinear systems: analysis, stability, and control},
  volume~10.
\newblock Springer, New York, N.Y., 1999.
\newblock ISBN 0-387-98513-1.
\end{thebibliography}


\end{document}      % End of the document
